{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d62010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dde5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziwen/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (18,20,26,38,43,44,45,46,48,53,60,61,62,63,64,65,69,70,72,73,84,85,86,87,88,89,92,94,97,103,104,105,106,107,113,115,129,130,131,132,133,134,142,143,144,145,146,147,154,155,156,157,158,159,164) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('Cohort2017-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5317cef",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/data/share/ziwen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13575/2585251820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcsv_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/data/share/ziwen'"
     ]
    }
   ],
   "source": [
    "#file_path = r\"D:\\Cornell\\Research\\data\\Cohort2017-2018.csv\"\n",
    "\n",
    "#chunk_size = 1000\n",
    "\n",
    "#csv_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "#chunks = []\n",
    "\n",
    "#for chunk in csv_iterator:\n",
    "    \n",
    "    #chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "#df1 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd56eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CO_SEQNUM</th>\n",
       "      <th>DOD_YY</th>\n",
       "      <th>DOB_YY</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>DOB_TT</th>\n",
       "      <th>DOB_WK</th>\n",
       "      <th>BFACIL</th>\n",
       "      <th>BFACIL3</th>\n",
       "      <th>MAGER</th>\n",
       "      <th>...</th>\n",
       "      <th>F_CA_DOWN</th>\n",
       "      <th>F_CA_DISOR</th>\n",
       "      <th>F_CA_HYPO</th>\n",
       "      <th>NO_CONGEN</th>\n",
       "      <th>ITRAN</th>\n",
       "      <th>ILIVE</th>\n",
       "      <th>BFED</th>\n",
       "      <th>F_BFED</th>\n",
       "      <th>FLGND</th>\n",
       "      <th>OEGest_R3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1135</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1426</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 CO_SEQNUM  DOD_YY  DOB_YY  DOB_MM  DOB_TT  DOB_WK  BFACIL  \\\n",
       "0           0               NaN    2019       1    1135       3       1   \n",
       "1           1               NaN    2019       1    1305       3       1   \n",
       "2           2               NaN    2019       1     800       3       1   \n",
       "3           3               NaN    2019       1     130       4       1   \n",
       "4           4               NaN    2019       1    1426       4       1   \n",
       "\n",
       "   BFACIL3  MAGER  ...  F_CA_DOWN  F_CA_DISOR  F_CA_HYPO  NO_CONGEN  ITRAN  \\\n",
       "0        1     29  ...          1           1          1          1      N   \n",
       "1        1     40  ...          1           1          1          1      N   \n",
       "2        1     30  ...          1           1          1          1      N   \n",
       "3        1     25  ...          1           1          1          1      N   \n",
       "4        1     38  ...          1           1          1          1      N   \n",
       "\n",
       "   ILIVE BFED F_BFED FLGND  OEGest_R3  \n",
       "0      Y    Y      1                1  \n",
       "1      Y    Y      1                2  \n",
       "2      Y    Y      1                2  \n",
       "3      Y    Y      1                2  \n",
       "4      Y    Y      1                2  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47688651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3801534"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a0128e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 15,  6,  2, 13, 14, 10, 99,  5,  9,  7, 12,  8, 11,  4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['MRACE15'].unique()\n",
    "df1['FRACE15'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ebdda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziwen/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (18,115) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('Cohort2018-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a050c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path2 = r\"D:\\Cornell\\Research\\data\\Cohort2018-2019.csv\"\n",
    "\n",
    "#chunk_size = 1000\n",
    "\n",
    "#csv_iterator = pd.read_csv(file_path2, chunksize=chunk_size)\n",
    "#chunks = []\n",
    "\n",
    "#for chunk in csv_iterator:\n",
    "    \n",
    "    #chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "#df2 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04202832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CO_SEQNUM</th>\n",
       "      <th>DOD_YY</th>\n",
       "      <th>DOB_YY</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>DOB_TT</th>\n",
       "      <th>DOB_WK</th>\n",
       "      <th>BFACIL</th>\n",
       "      <th>BFACIL3</th>\n",
       "      <th>MAGER</th>\n",
       "      <th>...</th>\n",
       "      <th>F_CA_DOWN</th>\n",
       "      <th>F_CA_DISOR</th>\n",
       "      <th>F_CA_HYPO</th>\n",
       "      <th>NO_CONGEN</th>\n",
       "      <th>ITRAN</th>\n",
       "      <th>ILIVE</th>\n",
       "      <th>BFED</th>\n",
       "      <th>F_BFED</th>\n",
       "      <th>FLGND</th>\n",
       "      <th>OEGest_R3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1135</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1426</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 CO_SEQNUM  DOD_YY  DOB_YY  DOB_MM  DOB_TT  DOB_WK  BFACIL  \\\n",
       "0           0               NaN    2019       1    1135       3       1   \n",
       "1           1               NaN    2019       1    1305       3       1   \n",
       "2           2               NaN    2019       1     800       3       1   \n",
       "3           3               NaN    2019       1     130       4       1   \n",
       "4           4               NaN    2019       1    1426       4       1   \n",
       "\n",
       "   BFACIL3  MAGER  ...  F_CA_DOWN  F_CA_DISOR  F_CA_HYPO  NO_CONGEN  ITRAN  \\\n",
       "0        1     29  ...          1           1          1          1      N   \n",
       "1        1     40  ...          1           1          1          1      N   \n",
       "2        1     30  ...          1           1          1          1      N   \n",
       "3        1     25  ...          1           1          1          1      N   \n",
       "4        1     38  ...          1           1          1          1      N   \n",
       "\n",
       "   ILIVE BFED F_BFED FLGND  OEGest_R3  \n",
       "0      Y    Y      1                1  \n",
       "1      Y    Y      1                2  \n",
       "2      Y    Y      1                2  \n",
       "3      Y    Y      1                2  \n",
       "4      Y    Y      1                2  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650a50c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3757582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e15db08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 15,  6,  2, 13, 14, 10, 99,  5,  9,  7, 12,  8, 11,  4],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['MRACE15'].unique()\n",
    "df2['FRACE15'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ec9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziwen/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('Cohort2019-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebaab921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path3 = r\"D:\\Cornell\\Research\\data\\Cohort2019-2020.csv\"\n",
    "\n",
    "#chunk_size = 1000\n",
    "\n",
    "#csv_iterator = pd.read_csv(file_path3, chunksize=chunk_size)\n",
    "#chunks = []\n",
    "\n",
    "#for chunk in csv_iterator:\n",
    "    \n",
    "    #chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "#df3 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25fa0ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CO_SEQNUM</th>\n",
       "      <th>DOD_YY</th>\n",
       "      <th>DOB_YY</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>DOB_TT</th>\n",
       "      <th>DOB_WK</th>\n",
       "      <th>BFACIL</th>\n",
       "      <th>BFACIL3</th>\n",
       "      <th>MAGER</th>\n",
       "      <th>...</th>\n",
       "      <th>F_CA_DOWN</th>\n",
       "      <th>F_CA_DISOR</th>\n",
       "      <th>F_CA_HYPO</th>\n",
       "      <th>NO_CONGEN</th>\n",
       "      <th>ITRAN</th>\n",
       "      <th>ILIVE</th>\n",
       "      <th>BFED</th>\n",
       "      <th>F_BFED</th>\n",
       "      <th>FLGND</th>\n",
       "      <th>OEGest_R3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1123</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1712</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2212</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>806</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 CO_SEQNUM  DOD_YY  DOB_YY  DOB_MM  DOB_TT  DOB_WK  BFACIL  \\\n",
       "0           0               NaN    2020       1    1123       4       1   \n",
       "1           1               NaN    2020       1      29       6       1   \n",
       "2           2               NaN    2020       1    1712       6       1   \n",
       "3           3               NaN    2020       1    2212       6       1   \n",
       "4           4               NaN    2020       1     806       6       1   \n",
       "\n",
       "   BFACIL3  MAGER  ...  F_CA_DOWN  F_CA_DISOR  F_CA_HYPO  NO_CONGEN  ITRAN  \\\n",
       "0        1     25  ...          1           1          1          1      N   \n",
       "1        1     28  ...          1           1          1          1      N   \n",
       "2        1     36  ...          1           1          1          1      N   \n",
       "3        1     32  ...          1           1          1          1      N   \n",
       "4        1     39  ...          1           1          1          1      N   \n",
       "\n",
       "  ILIVE BFED F_BFED FLGND  OEGest_R3  \n",
       "0     Y    Y      1                2  \n",
       "1     Y    Y      1                2  \n",
       "2     Y    Y      1                2  \n",
       "3     Y    Y      1                2  \n",
       "4     Y    Y      1                2  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e534657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3619826"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b583d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  2, 99, 10, 15,  6,  5, 13,  8,  4, 14, 12,  7, 11,  9],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['MRACE15'].unique()\n",
    "df1['FRACE15'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037d2d0",
   "metadata": {},
   "source": [
    "### one dataset for each category of the new variable - Parental Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33650299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Parental Ethnicity</th>\n",
       "      <th>MRACE15</th>\n",
       "      <th>FRACE15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Both White (only)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Both Black (only)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Both AIAN (only)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Both Asian Indian (only)</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Both Chinese (only)</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Both Filipino (only)</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Both Japanese (only)</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Both Korean (only)</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Both Vietnamese (only)</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Both Other Asian (only)</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Both Hawaiian (only)</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Both Guamanian (only)</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Both Samoan (only)</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Both Other Pacific Islander (only)</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Both More than one race</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Mother White (only) and Father non-white</td>\n",
       "      <td>1</td>\n",
       "      <td>any except 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Mother Black (only) and Father non-black</td>\n",
       "      <td>2</td>\n",
       "      <td>any except 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Mother AIAN (only) and Father non-AIAN</td>\n",
       "      <td>3</td>\n",
       "      <td>any except 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Mother Asian Indian (only) and Father non-Asian</td>\n",
       "      <td>4</td>\n",
       "      <td>any except 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Mother Chinese (only) and Father non-Chinese</td>\n",
       "      <td>5</td>\n",
       "      <td>any except 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Mother Filipino (only) and Father non-Filipino</td>\n",
       "      <td>6</td>\n",
       "      <td>any except 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Mother Japanese (only) and Father non-Japanese</td>\n",
       "      <td>7</td>\n",
       "      <td>any except 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Mother Korean (only) and Father non-Korean</td>\n",
       "      <td>8</td>\n",
       "      <td>any except 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Mother Vietnamese (only) and Father non-Vietna...</td>\n",
       "      <td>9</td>\n",
       "      <td>any except 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Mother Other Asian (only) and Father non-Other...</td>\n",
       "      <td>10</td>\n",
       "      <td>any except 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Mother Hawaiian (only) and Father non-Hawaiian</td>\n",
       "      <td>11</td>\n",
       "      <td>any except 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Mother Guamanian (only) and Father non-Guamanian</td>\n",
       "      <td>12</td>\n",
       "      <td>any except 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Mother Samoan (only) and Father non-Samoan</td>\n",
       "      <td>13</td>\n",
       "      <td>any except 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Mother Other Pacific Islander (only) and Fathe...</td>\n",
       "      <td>14</td>\n",
       "      <td>any except 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Mother More than one race and Father non-More ...</td>\n",
       "      <td>15</td>\n",
       "      <td>any except 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category                                 Parental Ethnicity  MRACE15  \\\n",
       "0          1                                  Both White (only)        1   \n",
       "1          2                                  Both Black (only)        2   \n",
       "2          3                                   Both AIAN (only)        3   \n",
       "3          4                           Both Asian Indian (only)        4   \n",
       "4          5                                Both Chinese (only)        5   \n",
       "5          6                               Both Filipino (only)        6   \n",
       "6          7                               Both Japanese (only)        7   \n",
       "7          8                                 Both Korean (only)        8   \n",
       "8          9                             Both Vietnamese (only)        9   \n",
       "9         10                            Both Other Asian (only)       10   \n",
       "10        11                               Both Hawaiian (only)       11   \n",
       "11        12                              Both Guamanian (only)       12   \n",
       "12        13                                 Both Samoan (only)       13   \n",
       "13        14                 Both Other Pacific Islander (only)       14   \n",
       "14        15                            Both More than one race       15   \n",
       "15        16           Mother White (only) and Father non-white        1   \n",
       "16        17           Mother Black (only) and Father non-black        2   \n",
       "17        18             Mother AIAN (only) and Father non-AIAN        3   \n",
       "18        19    Mother Asian Indian (only) and Father non-Asian        4   \n",
       "19        20       Mother Chinese (only) and Father non-Chinese        5   \n",
       "20        21     Mother Filipino (only) and Father non-Filipino        6   \n",
       "21        22     Mother Japanese (only) and Father non-Japanese        7   \n",
       "22        23         Mother Korean (only) and Father non-Korean        8   \n",
       "23        24  Mother Vietnamese (only) and Father non-Vietna...        9   \n",
       "24        25  Mother Other Asian (only) and Father non-Other...       10   \n",
       "25        26     Mother Hawaiian (only) and Father non-Hawaiian       11   \n",
       "26        27   Mother Guamanian (only) and Father non-Guamanian       12   \n",
       "27        28         Mother Samoan (only) and Father non-Samoan       13   \n",
       "28        29  Mother Other Pacific Islander (only) and Fathe...       14   \n",
       "29        30  Mother More than one race and Father non-More ...       15   \n",
       "\n",
       "          FRACE15  \n",
       "0               1  \n",
       "1               2  \n",
       "2               3  \n",
       "3               4  \n",
       "4               5  \n",
       "5               6  \n",
       "6               7  \n",
       "7               8  \n",
       "8               9  \n",
       "9              10  \n",
       "10             11  \n",
       "11             12  \n",
       "12             13  \n",
       "13             14  \n",
       "14             15  \n",
       "15   any except 1  \n",
       "16   any except 2  \n",
       "17   any except 3  \n",
       "18   any except 4  \n",
       "19   any except 5  \n",
       "20   any except 6  \n",
       "21   any except 7  \n",
       "22   any except 8  \n",
       "23   any except 9  \n",
       "24  any except 10  \n",
       "25  any except 11  \n",
       "26  any except 12  \n",
       "27  any except 13  \n",
       "28  any except 14  \n",
       "29  any except 15  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parental_ethnicity = pd.DataFrame(\n",
    "    {\n",
    "        'Category' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n",
    "                      16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \n",
    "        'Parental Ethnicity' : ['Both White (only)', \n",
    "                                'Both Black (only)', \n",
    "                                'Both AIAN (only)', \n",
    "                                'Both Asian Indian (only)', \n",
    "                                'Both Chinese (only)', \n",
    "                                'Both Filipino (only)', \n",
    "                                'Both Japanese (only)', \n",
    "                                'Both Korean (only)', \n",
    "                                'Both Vietnamese (only)', \n",
    "                                'Both Other Asian (only)', \n",
    "                                'Both Hawaiian (only)', \n",
    "                                'Both Guamanian (only)', \n",
    "                                'Both Samoan (only)', \n",
    "                                'Both Other Pacific Islander (only)', \n",
    "                                'Both More than one race', \n",
    "                                'Mother White (only) and Father non-white', \n",
    "                                'Mother Black (only) and Father non-black', \n",
    "                                'Mother AIAN (only) and Father non-AIAN', \n",
    "                                'Mother Asian Indian (only) and Father non-Asian', \n",
    "                                'Mother Chinese (only) and Father non-Chinese', \n",
    "                                'Mother Filipino (only) and Father non-Filipino', \n",
    "                                'Mother Japanese (only) and Father non-Japanese', \n",
    "                                'Mother Korean (only) and Father non-Korean', \n",
    "                                'Mother Vietnamese (only) and Father non-Vietnamese', \n",
    "                                'Mother Other Asian (only) and Father non-Other Asian', \n",
    "                                'Mother Hawaiian (only) and Father non-Hawaiian', \n",
    "                                'Mother Guamanian (only) and Father non-Guamanian', \n",
    "                                'Mother Samoan (only) and Father non-Samoan', \n",
    "                                'Mother Other Pacific Islander (only) and Father non-Other Pacific Islander', \n",
    "                                'Mother More than one race and Father non-More than one race'], \n",
    "        'MRACE15' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n",
    "        'FRACE15' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 'any except 1', 'any except 2', 'any except 3', \n",
    "                     'any except 4', 'any except 5', 'any except 6', 'any except 7', 'any except 8', 'any except 9', \n",
    "                     'any except 10', 'any except 11', 'any except 12', 'any except 13', 'any except 14', 'any except 15']\n",
    "    }\n",
    ")\n",
    "\n",
    "parental_ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51d108",
   "metadata": {},
   "source": [
    "### Create 30 separate datasets for each catrgory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e664115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c1 = df1.loc[(df1['MRACE15'] == 1) & (df1['FRACE15'] == 1)]\n",
    "df1_c1.to_csv('D:\\Cornell\\Research\\data\\df1_c1.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f876b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c1 = df2.loc[(df2['MRACE15'] == 1) & (df2['FRACE15'] == 1)]\n",
    "df2_c1.to_csv('D:\\Cornell\\Research\\data\\df2_c1.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef798d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c1 = df3.loc[(df3['MRACE15'] == 1) & (df3['FRACE15'] == 1)]\n",
    "df3_c1.to_csv('D:\\Cornell\\Research\\data\\df3_c1.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d64341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c1 = r\"D:\\Cornell\\Research\\data\\df1_c1.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df1_c1, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df1_c1 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bb51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df2_c1 = r\"D:\\Cornell\\Research\\data\\df2_c1.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df2_c1, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df2_c1 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb0c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df3_c1 = r\"D:\\Cornell\\Research\\data\\df3_c1.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df3_c1, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df3_c1 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f3df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the three datasets\n",
    "c1_final = pd.concat([df1_c1, df2_c1, df3_c1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffb454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_final.to_csv('D:\\Cornell\\Research\\data\\c1_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59816056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200497"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length\n",
    "len(df1_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adcc3c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2148911"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32478a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045759"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75044168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6395167"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c1_final) # equivalent to the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above steps to create final datasets from 2-30..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6654eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c2 = df1.loc[(df1['MRACE15'] == 2) & (df1['FRACE15'] == 2)]\n",
    "df1_c2.to_csv('D:\\Cornell\\Research\\data\\df1_c2.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0186d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c2 = df2.loc[(df2['MRACE15'] == 2) & (df2['FRACE15'] == 2)]\n",
    "df2_c2.to_csv('D:\\Cornell\\Research\\data\\df2_c2.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f6c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c2 = df3.loc[(df3['MRACE15'] == 2) & (df3['FRACE15'] == 2)]\n",
    "df3_c2.to_csv('D:\\Cornell\\Research\\data\\df3_c2.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0ab14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c2 = r\"D:\\Cornell\\Research\\data\\df1_c2.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df1_c2, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df1_c2 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e185391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df2_c2 = r\"D:\\Cornell\\Research\\data\\df2_c2.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df2_c2, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df2_c2 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d94ba105",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df3_c2 = r\"D:\\Cornell\\Research\\data\\df3_c2.csv\"\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "csv_iterator = pd.read_csv(path_df3_c2, chunksize=chunk_size)\n",
    "chunks = []\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    \n",
    "df3_c2 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b3f0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the three datasets\n",
    "c2_final = pd.concat([df1_c2, df2_c2, df3_c2], ignore_index = True)\n",
    "c2_final.to_csv('D:\\Cornell\\Research\\data\\c2_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed0d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01406f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c3 = df1.loc[(df1['MRACE15'] == 3) & (df1['FRACE15'] == 3)]\n",
    "df1_c3.to_csv('D:\\Cornell\\Research\\data\\df1_c3.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056070eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c3 = df2.loc[(df2['MRACE15'] == 3) & (df2['FRACE15'] == 3)]\n",
    "df2_c3.to_csv('D:\\Cornell\\Research\\data\\df2_c3.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa9f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c3 = df3.loc[(df3['MRACE15'] == 3) & (df3['FRACE15'] == 3)]\n",
    "df3_c3.to_csv('D:\\Cornell\\Research\\data\\df3_c3.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "657f5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c3 = r\"D:\\Cornell\\Research\\data\\df1_c3.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c3, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c3 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c3 = r\"D:\\Cornell\\Research\\data\\df2_c3.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c3, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c3 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c3 = r\"D:\\Cornell\\Research\\data\\df3_c3.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c3, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c3 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "890b5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the three datasets\n",
    "c3_final = pd.concat([df1_c3, df2_c3, df3_c3], ignore_index = True)\n",
    "c3_final.to_csv('D:\\Cornell\\Research\\data\\c3_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8e4cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13174"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1_c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa0a0d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13070"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2_c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b437707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11760"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3_c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60d15674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c3_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb1dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685092f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c4 = df1.loc[(df1['MRACE15'] == 4) & (df1['FRACE15'] == 4)]\n",
    "df1_c4.to_csv('D:\\Cornell\\Research\\data\\df1_c4.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a621cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c4 = df2.loc[(df2['MRACE15'] == 4) & (df2['FRACE15'] == 4)]\n",
    "df2_c4.to_csv('D:\\Cornell\\Research\\data\\df2_c4.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfea6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c4 = df3.loc[(df3['MRACE15'] == 4) & (df3['FRACE15'] == 4)]\n",
    "df3_c4.to_csv('D:\\Cornell\\Research\\data\\df3_c4.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "700a1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c4 = r\"D:\\Cornell\\Research\\data\\df1_c4.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c4, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c4 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c4 = r\"D:\\Cornell\\Research\\data\\df2_c4.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c4, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c4 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c4 = r\"D:\\Cornell\\Research\\data\\df3_c4.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c4, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c4 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c4_final = pd.concat([df1_c4, df2_c4, df3_c4], ignore_index = True)\n",
    "c4_final.to_csv('D:\\Cornell\\Research\\data\\c4_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcec306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82881971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c5 = df1.loc[(df1['MRACE15'] == 5) & (df1['FRACE15'] == 5)]\n",
    "df1_c5.to_csv('D:\\Cornell\\Research\\data\\df1_c5.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacad4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c5 = df2.loc[(df2['MRACE15'] == 5) & (df2['FRACE15'] == 5)]\n",
    "df2_c5.to_csv('D:\\Cornell\\Research\\data\\df2_c5.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37b67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c5 = df3.loc[(df3['MRACE15'] == 5) & (df3['FRACE15'] == 5)]\n",
    "df3_c5.to_csv('D:\\Cornell\\Research\\data\\df3_c5.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8a4af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c5 = r\"D:\\Cornell\\Research\\data\\df1_c5.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c5, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c5 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c5 = r\"D:\\Cornell\\Research\\data\\df2_c5.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c5, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c5 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c5 = r\"D:\\Cornell\\Research\\data\\df3_c5.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c5, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c5 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c5_final = pd.concat([df1_c5, df2_c5, df3_c5], ignore_index = True)\n",
    "c5_final.to_csv('D:\\Cornell\\Research\\data\\c5_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5dd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc659c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c6 = df1.loc[(df1['MRACE15'] == 6) & (df1['FRACE15'] == 6)]\n",
    "df1_c6.to_csv('D:\\Cornell\\Research\\data\\df1_c6.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d64a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c6 = df2.loc[(df2['MRACE15'] == 6) & (df2['FRACE15'] == 6)]\n",
    "df2_c6.to_csv('D:\\Cornell\\Research\\data\\df2_c6.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c6 = df3.loc[(df3['MRACE15'] == 6) & (df3['FRACE15'] == 6)]\n",
    "df3_c6.to_csv('D:\\Cornell\\Research\\data\\df3_c6.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d7af7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c6 = r\"D:\\Cornell\\Research\\data\\df1_c6.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c6, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c6 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c6 = r\"D:\\Cornell\\Research\\data\\df2_c6.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c6, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c6 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c6 = r\"D:\\Cornell\\Research\\data\\df3_c6.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c6, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c6 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c6_final = pd.concat([df1_c6, df2_c6, df3_c6], ignore_index = True)\n",
    "c6_final.to_csv('D:\\Cornell\\Research\\data\\c6_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da9a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c7 = df1.loc[(df1['MRACE15'] == 7) & (df1['FRACE15'] == 7)]\n",
    "df1_c7.to_csv('D:\\Cornell\\Research\\data\\df1_c7.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55f7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c7 = df2.loc[(df2['MRACE15'] == 7) & (df2['FRACE15'] == 7)]\n",
    "df2_c7.to_csv('D:\\Cornell\\Research\\data\\df2_c7.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b0d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c7 = df3.loc[(df3['MRACE15'] == 7) & (df3['FRACE15'] == 7)]\n",
    "df3_c7.to_csv('D:\\Cornell\\Research\\data\\df3_c7.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "201c4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c7 = r\"D:\\Cornell\\Research\\data\\df1_c7.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c7, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c7 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c7 = r\"D:\\Cornell\\Research\\data\\df2_c7.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c7, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c7 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c7 = r\"D:\\Cornell\\Research\\data\\df3_c7.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c7, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c7 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c7_final = pd.concat([df1_c7, df2_c7, df3_c7], ignore_index = True)\n",
    "c7_final.to_csv('D:\\Cornell\\Research\\data\\c7_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b05ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f53a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c8 = df1.loc[(df1['MRACE15'] == 8) & (df1['FRACE15'] == 8)]\n",
    "df1_c8.to_csv('D:\\Cornell\\Research\\data\\df1_c8.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5104684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c8 = df2.loc[(df2['MRACE15'] == 8) & (df2['FRACE15'] == 8)]\n",
    "df2_c8.to_csv('D:\\Cornell\\Research\\data\\df2_c8.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f508e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c8 = df3.loc[(df3['MRACE15'] == 8) & (df3['FRACE15'] == 8)]\n",
    "df3_c8.to_csv('D:\\Cornell\\Research\\data\\df3_c8.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0274474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c8 = r\"D:\\Cornell\\Research\\data\\df1_c8.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c8, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c8 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c8 = r\"D:\\Cornell\\Research\\data\\df2_c8.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c8, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c8 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c8 = r\"D:\\Cornell\\Research\\data\\df3_c8.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c8, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c8 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c8_final = pd.concat([df1_c8, df2_c8, df3_c8], ignore_index = True)\n",
    "c8_final.to_csv('D:\\Cornell\\Research\\data\\c8_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0906d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8aa408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c9 = df1.loc[(df1['MRACE15'] == 9) & (df1['FRACE15'] == 9)]\n",
    "df1_c9.to_csv('D:\\Cornell\\Research\\data\\df1_c9.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec71a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c9 = df2.loc[(df2['MRACE15'] == 9) & (df2['FRACE15'] == 9)]\n",
    "df2_c9.to_csv('D:\\Cornell\\Research\\data\\df2_c9.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86f2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c9 = df3.loc[(df3['MRACE15'] == 9) & (df3['FRACE15'] == 9)]\n",
    "df3_c9.to_csv('D:\\Cornell\\Research\\data\\df3_c9.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02202eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c9 = r\"D:\\Cornell\\Research\\data\\df1_c9.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c9, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c9 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c9 = r\"D:\\Cornell\\Research\\data\\df2_c9.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c9, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c9 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c9 = r\"D:\\Cornell\\Research\\data\\df3_c9.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c9, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c9 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c9_final = pd.concat([df1_c9, df2_c9, df3_c9], ignore_index = True)\n",
    "c9_final.to_csv('D:\\Cornell\\Research\\data\\c9_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2414859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f27f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c10 = df1.loc[(df1['MRACE15'] == 10) & (df1['FRACE15'] == 10)]\n",
    "df1_c10.to_csv('D:\\Cornell\\Research\\data\\df1_c10.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8588fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c10 = df2.loc[(df2['MRACE15'] == 10) & (df2['FRACE15'] == 10)]\n",
    "df2_c10.to_csv('D:\\Cornell\\Research\\data\\df2_c10.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e56ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c10 = df3.loc[(df3['MRACE15'] == 10) & (df3['FRACE15'] == 10)]\n",
    "df3_c10.to_csv('D:\\Cornell\\Research\\data\\df3_c10.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf146e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c10 = r\"D:\\Cornell\\Research\\data\\df1_c10.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c10, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c10 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c10 = r\"D:\\Cornell\\Research\\data\\df2_c10.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c10, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c10 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c10 = r\"D:\\Cornell\\Research\\data\\df3_c10.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c10, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c10 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c10_final = pd.concat([df1_c10, df2_c10, df3_c10], ignore_index = True)\n",
    "c10_final.to_csv('D:\\Cornell\\Research\\data\\c10_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdda99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152ca537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c11 = df1.loc[(df1['MRACE15'] == 11) & (df1['FRACE15'] == 11)]\n",
    "df1_c11.to_csv('D:\\Cornell\\Research\\data\\df1_c11.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ca2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c11 = df2.loc[(df2['MRACE15'] == 11) & (df2['FRACE15'] == 11)]\n",
    "df2_c11.to_csv('D:\\Cornell\\Research\\data\\df2_c11.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5e618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c11 = df3.loc[(df3['MRACE15'] == 11) & (df3['FRACE15'] == 11)]\n",
    "df3_c11.to_csv('D:\\Cornell\\Research\\data\\df3_c11.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60202cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c11 = r\"D:\\Cornell\\Research\\data\\df1_c11.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c11, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c11 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c11 = r\"D:\\Cornell\\Research\\data\\df2_c11.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c11, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c11 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c11 = r\"D:\\Cornell\\Research\\data\\df3_c11.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c11, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c11 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c11_final = pd.concat([df1_c11, df2_c11, df3_c11], ignore_index = True)\n",
    "c11_final.to_csv('D:\\Cornell\\Research\\data\\c11_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142079e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348aeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c12 = df1.loc[(df1['MRACE15'] == 12) & (df1['FRACE15'] == 12)]\n",
    "df1_c12.to_csv('D:\\Cornell\\Research\\data\\df1_c12.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864347a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c12 = df2.loc[(df2['MRACE15'] == 12) & (df2['FRACE15'] == 12)]\n",
    "df2_c12.to_csv('D:\\Cornell\\Research\\data\\df2_c12.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6fe449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c12 = df3.loc[(df3['MRACE15'] == 12) & (df3['FRACE15'] == 12)]\n",
    "df3_c12.to_csv('D:\\Cornell\\Research\\data\\df3_c12.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15f68a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c12 = r\"D:\\Cornell\\Research\\data\\df1_c12.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c12, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c12 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c12 = r\"D:\\Cornell\\Research\\data\\df2_c12.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c12, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c12 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c12 = r\"D:\\Cornell\\Research\\data\\df3_c12.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c12, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c12 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c12_final = pd.concat([df1_c12, df2_c12, df3_c12], ignore_index = True)\n",
    "c12_final.to_csv('D:\\Cornell\\Research\\data\\c12_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf8092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a11f1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c13 = df1.loc[(df1['MRACE15'] == 13) & (df1['FRACE15'] == 13)]\n",
    "df1_c13.to_csv('D:\\Cornell\\Research\\data\\df1_c13.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755dccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c13 = df2.loc[(df2['MRACE15'] == 13) & (df2['FRACE15'] == 13)]\n",
    "df2_c13.to_csv('D:\\Cornell\\Research\\data\\df2_c13.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e72bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c13 = df3.loc[(df3['MRACE15'] == 13) & (df3['FRACE15'] == 13)]\n",
    "df3_c13.to_csv('D:\\Cornell\\Research\\data\\df3_c13.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4800de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c13 = r\"D:\\Cornell\\Research\\data\\df1_c13.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c13, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c13 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c13 = r\"D:\\Cornell\\Research\\data\\df2_c13.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c13, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c13 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c13 = r\"D:\\Cornell\\Research\\data\\df3_c13.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c13, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c13 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c13_final = pd.concat([df1_c13, df2_c13, df3_c13], ignore_index = True)\n",
    "c13_final.to_csv('D:\\Cornell\\Research\\data\\c13_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464aa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07e9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c14 = df1.loc[(df1['MRACE15'] == 14) & (df1['FRACE15'] == 14)]\n",
    "df1_c14.to_csv('D:\\Cornell\\Research\\data\\df1_c14.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4ac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c14 = df2.loc[(df2['MRACE15'] == 14) & (df2['FRACE15'] == 14)]\n",
    "df2_c14.to_csv('D:\\Cornell\\Research\\data\\df2_c14.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a77291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c14 = df3.loc[(df3['MRACE15'] == 14) & (df3['FRACE15'] == 14)]\n",
    "df3_c14.to_csv('D:\\Cornell\\Research\\data\\df3_c14.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9669db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c14 = r\"D:\\Cornell\\Research\\data\\df1_c14.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c14, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c14 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c14 = r\"D:\\Cornell\\Research\\data\\df2_c14.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c14, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c14 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c14 = r\"D:\\Cornell\\Research\\data\\df3_c14.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c14, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c14 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c14_final = pd.concat([df1_c14, df2_c14, df3_c14], ignore_index = True)\n",
    "c14_final.to_csv('D:\\Cornell\\Research\\data\\c14_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83241ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a802d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c15 = df1.loc[(df1['MRACE15'] == 15) & (df1['FRACE15'] == 15)]\n",
    "df1_c15.to_csv('D:\\Cornell\\Research\\data\\df1_c15.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53be898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c15 = df2.loc[(df2['MRACE15'] == 15) & (df2['FRACE15'] == 15)]\n",
    "df2_c15.to_csv('D:\\Cornell\\Research\\data\\df2_c15.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d2991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c15 = df3.loc[(df3['MRACE15'] == 15) & (df3['FRACE15'] == 15)]\n",
    "df3_c15.to_csv('D:\\Cornell\\Research\\data\\df3_c15.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e6e171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c15 = r\"D:\\Cornell\\Research\\data\\df1_c15.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c15, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c15 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c15 = r\"D:\\Cornell\\Research\\data\\df2_c15.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c15, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c15 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c15 = r\"D:\\Cornell\\Research\\data\\df3_c15.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c15, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c15 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c15_final = pd.concat([df1_c15, df2_c15, df3_c15], ignore_index = True)\n",
    "c15_final.to_csv('D:\\Cornell\\Research\\data\\c15_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127edfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d938acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c16 = df1.loc[(df1['MRACE15'] == 1) & (df1['FRACE15'] != 1) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c16.to_csv('D:\\Cornell\\Research\\data\\df1_c16.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da729973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c16 = df2.loc[(df2['MRACE15'] == 1) & (df2['FRACE15'] != 1) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c16.to_csv('D:\\Cornell\\Research\\data\\df2_c16.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ca7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c16 = df3.loc[(df3['MRACE15'] == 1) & (df3['FRACE15'] != 1) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c16.to_csv('D:\\Cornell\\Research\\data\\df3_c16.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13dcb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c16 = r\"D:\\Cornell\\Research\\data\\df1_c16.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c16, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c16 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c16 = r\"D:\\Cornell\\Research\\data\\df2_c16.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c16, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c16 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c16 = r\"D:\\Cornell\\Research\\data\\df3_c16.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c16, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c16 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c16_final = pd.concat([df1_c16, df2_c16, df3_c16], ignore_index = True)\n",
    "c16_final.to_csv('D:\\Cornell\\Research\\data\\c16_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1536203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c17 = df1.loc[(df1['MRACE15'] == 2) & (df1['FRACE15'] != 2) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c17.to_csv('D:\\Cornell\\Research\\data\\df1_c17.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99085bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c17 = df2.loc[(df2['MRACE15'] == 2) & (df2['FRACE15'] != 2) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c17.to_csv('D:\\Cornell\\Research\\data\\df2_c17.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09dd758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c17 = df3.loc[(df3['MRACE15'] == 2) & (df3['FRACE15'] != 2) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c17.to_csv('D:\\Cornell\\Research\\data\\df3_c17.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1a17d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c17 = r\"D:\\Cornell\\Research\\data\\df1_c17.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c17, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c17 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c17 = r\"D:\\Cornell\\Research\\data\\df2_c17.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c17, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c17 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c17 = r\"D:\\Cornell\\Research\\data\\df3_c17.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c17, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c17 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c17_final = pd.concat([df1_c17, df2_c17, df3_c17], ignore_index = True)\n",
    "c17_final.to_csv('D:\\Cornell\\Research\\data\\c17_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc61413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826509de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c18 = df1.loc[(df1['MRACE15'] == 3) & (df1['FRACE15'] != 3) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c18.to_csv('D:\\Cornell\\Research\\data\\df1_c18.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "292bc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c18 = df2.loc[(df2['MRACE15'] == 3) & (df2['FRACE15'] != 3) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c18.to_csv('D:\\Cornell\\Research\\data\\df2_c18.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a33e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c18 = df3.loc[(df3['MRACE15'] == 3) & (df3['FRACE15'] != 3) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c18.to_csv('D:\\Cornell\\Research\\data\\df3_c18.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73c0c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c18 = r\"D:\\Cornell\\Research\\data\\df1_c18.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c18, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c18 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c18 = r\"D:\\Cornell\\Research\\data\\df2_c18.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c18, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c18 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c18 = r\"D:\\Cornell\\Research\\data\\df3_c18.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c18, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c18 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c18_final = pd.concat([df1_c18, df2_c18, df3_c18], ignore_index = True)\n",
    "c18_final.to_csv('D:\\Cornell\\Research\\data\\c18_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d49a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f400e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c19 = df1.loc[(df1['MRACE15'] == 4) & (df1['FRACE15'] != 4) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c19.to_csv('D:\\Cornell\\Research\\data\\df1_c19.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337bf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c19 = df2.loc[(df2['MRACE15'] == 4) & (df2['FRACE15'] != 4) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c19.to_csv('D:\\Cornell\\Research\\data\\df2_c19.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147f495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c19 = df3.loc[(df3['MRACE15'] == 4) & (df3['FRACE15'] != 4) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c19.to_csv('D:\\Cornell\\Research\\data\\df3_c19.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3947d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c19 = r\"D:\\Cornell\\Research\\data\\df1_c19.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c19, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c19 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c19 = r\"D:\\Cornell\\Research\\data\\df2_c19.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c19, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c19 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c19 = r\"D:\\Cornell\\Research\\data\\df3_c19.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c19, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c19 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c19_final = pd.concat([df1_c19, df2_c19, df3_c19], ignore_index = True)\n",
    "c19_final.to_csv('D:\\Cornell\\Research\\data\\c19_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b33a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd5a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c20 = df1.loc[(df1['MRACE15'] == 5) & (df1['FRACE15'] != 5) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c20.to_csv('D:\\Cornell\\Research\\data\\df1_c20.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b9e9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c20 = df2.loc[(df2['MRACE15'] == 5) & (df2['FRACE15'] != 5) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c20.to_csv('D:\\Cornell\\Research\\data\\df2_c20.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb636fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c20 = df3.loc[(df3['MRACE15'] == 5) & (df3['FRACE15'] != 5) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c20.to_csv('D:\\Cornell\\Research\\data\\df3_c20.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "007dc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c20 = r\"D:\\Cornell\\Research\\data\\df1_c20.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c20, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c20 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c20 = r\"D:\\Cornell\\Research\\data\\df2_c20.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c20, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c20 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c20 = r\"D:\\Cornell\\Research\\data\\df3_c20.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c20, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c20 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c20_final = pd.concat([df1_c20, df2_c20, df3_c20], ignore_index = True)\n",
    "c20_final.to_csv('D:\\Cornell\\Research\\data\\c20_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cd86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3cea148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c21 = df1.loc[(df1['MRACE15'] == 6) & (df1['FRACE15'] != 6) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c21.to_csv('D:\\Cornell\\Research\\data\\df1_c21.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "308627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c21 = df2.loc[(df2['MRACE15'] == 6) & (df2['FRACE15'] != 6) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c21.to_csv('D:\\Cornell\\Research\\data\\df2_c21.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed5d6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c21 = df3.loc[(df3['MRACE15'] == 6) & (df3['FRACE15'] != 6) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c21.to_csv('D:\\Cornell\\Research\\data\\df3_c21.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c501d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c21 = r\"D:\\Cornell\\Research\\data\\df1_c21.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c21, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c21 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c21 = r\"D:\\Cornell\\Research\\data\\df2_c21.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c21, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c21 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c21 = r\"D:\\Cornell\\Research\\data\\df3_c21.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c21, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c21 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c21_final = pd.concat([df1_c21, df2_c21, df3_c21], ignore_index = True)\n",
    "c21_final.to_csv('D:\\Cornell\\Research\\data\\c21_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73ab6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c22 = df1.loc[(df1['MRACE15'] == 7) & (df1['FRACE15'] != 7) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c22.to_csv('D:\\Cornell\\Research\\data\\df1_c22.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44cb0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c22 = df2.loc[(df2['MRACE15'] == 7) & (df2['FRACE15'] != 7) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c22.to_csv('D:\\Cornell\\Research\\data\\df2_c22.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a636681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c22 = df3.loc[(df3['MRACE15'] == 7) & (df3['FRACE15'] != 7) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c22.to_csv('D:\\Cornell\\Research\\data\\df3_c22.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8feb78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c22 = r\"D:\\Cornell\\Research\\data\\df1_c22.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c22, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c22 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c22 = r\"D:\\Cornell\\Research\\data\\df2_c22.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c22, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c22 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c22 = r\"D:\\Cornell\\Research\\data\\df3_c22.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c22, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c22 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c22_final = pd.concat([df1_c22, df2_c22, df3_c22], ignore_index = True)\n",
    "c22_final.to_csv('D:\\Cornell\\Research\\data\\c22_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34afc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "015b0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c23 = df1.loc[(df1['MRACE15'] == 8) & (df1['FRACE15'] != 8) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c23.to_csv('D:\\Cornell\\Research\\data\\df1_c23.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "355ae343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c23 = df2.loc[(df2['MRACE15'] == 8) & (df2['FRACE15'] != 8) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c23.to_csv('D:\\Cornell\\Research\\data\\df2_c23.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7971e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c23 = df3.loc[(df3['MRACE15'] == 8) & (df3['FRACE15'] != 8) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c23.to_csv('D:\\Cornell\\Research\\data\\df3_c23.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c2708a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c23 = r\"D:\\Cornell\\Research\\data\\df1_c23.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c23, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c23 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c23 = r\"D:\\Cornell\\Research\\data\\df2_c23.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c23, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c23 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c23 = r\"D:\\Cornell\\Research\\data\\df3_c23.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c23, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c23 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c23_final = pd.concat([df1_c23, df2_c23, df3_c23], ignore_index = True)\n",
    "c23_final.to_csv('D:\\Cornell\\Research\\data\\c23_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5bed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fedaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c24 = df1.loc[(df1['MRACE15'] == 9) & (df1['FRACE15'] != 9) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c24.to_csv('D:\\Cornell\\Research\\data\\df1_c24.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78afdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c24 = df2.loc[(df2['MRACE15'] == 9) & (df2['FRACE15'] != 9) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c24.to_csv('D:\\Cornell\\Research\\data\\df2_c24.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4983e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c24 = df3.loc[(df3['MRACE15'] == 9) & (df3['FRACE15'] != 9) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c24.to_csv('D:\\Cornell\\Research\\data\\df3_c24.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "108fc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c24 = r\"D:\\Cornell\\Research\\data\\df1_c24.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c24, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c24 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c24 = r\"D:\\Cornell\\Research\\data\\df2_c24.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c24, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c24 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c24 = r\"D:\\Cornell\\Research\\data\\df3_c24.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c24, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c24 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c24_final = pd.concat([df1_c24, df2_c24, df3_c24], ignore_index = True)\n",
    "c24_final.to_csv('D:\\Cornell\\Research\\data\\c24_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af3fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d383c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c25 = df1.loc[(df1['MRACE15'] == 10) & (df1['FRACE15'] != 10) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c25.to_csv('D:\\Cornell\\Research\\data\\df1_c25.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ad7857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c25 = df2.loc[(df2['MRACE15'] == 10) & (df2['FRACE15'] != 10) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c25.to_csv('D:\\Cornell\\Research\\data\\df2_c25.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30bf52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c25 = df3.loc[(df3['MRACE15'] == 10) & (df3['FRACE15'] != 10) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c25.to_csv('D:\\Cornell\\Research\\data\\df3_c25.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e627422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c25 = r\"D:\\Cornell\\Research\\data\\df1_c25.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c25, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c25 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c25 = r\"D:\\Cornell\\Research\\data\\df2_c25.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c25, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c25 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c25 = r\"D:\\Cornell\\Research\\data\\df3_c25.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c25, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c25 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c25_final = pd.concat([df1_c25, df2_c25, df3_c25], ignore_index = True)\n",
    "c25_final.to_csv('D:\\Cornell\\Research\\data\\c25_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c73d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4567f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c26 = df1.loc[(df1['MRACE15'] == 11) & (df1['FRACE15'] != 11) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c26.to_csv('D:\\Cornell\\Research\\data\\df1_c26.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a82cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c26 = df2.loc[(df2['MRACE15'] == 11) & (df2['FRACE15'] != 11) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c26.to_csv('D:\\Cornell\\Research\\data\\df2_c26.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5228ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c26 = df3.loc[(df3['MRACE15'] == 11) & (df3['FRACE15'] != 11) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c26.to_csv('D:\\Cornell\\Research\\data\\df3_c26.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "679e49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c26 = r\"D:\\Cornell\\Research\\data\\df1_c26.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c26, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c26 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c26 = r\"D:\\Cornell\\Research\\data\\df2_c26.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c26, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c26 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c26 = r\"D:\\Cornell\\Research\\data\\df3_c26.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c26, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c26 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c26_final = pd.concat([df1_c26, df2_c26, df3_c26], ignore_index = True)\n",
    "c26_final.to_csv('D:\\Cornell\\Research\\data\\c26_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3183de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8eb6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c27 = df1.loc[(df1['MRACE15'] == 12) & (df1['FRACE15'] != 12) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c27.to_csv('D:\\Cornell\\Research\\data\\df1_c27.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d68ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c27 = df2.loc[(df2['MRACE15'] == 12) & (df2['FRACE15'] != 12) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c27.to_csv('D:\\Cornell\\Research\\data\\df2_c27.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c969729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c27 = df3.loc[(df3['MRACE15'] == 12) & (df3['FRACE15'] != 12) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c27.to_csv('D:\\Cornell\\Research\\data\\df3_c27.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb88a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c27 = r\"D:\\Cornell\\Research\\data\\df1_c27.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c27, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c27 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c27 = r\"D:\\Cornell\\Research\\data\\df2_c27.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c27, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c27 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c27 = r\"D:\\Cornell\\Research\\data\\df3_c27.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c27, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c27 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c27_final = pd.concat([df1_c27, df2_c27, df3_c27], ignore_index = True)\n",
    "c27_final.to_csv('D:\\Cornell\\Research\\data\\c27_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e1ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4519c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c28 = df1.loc[(df1['MRACE15'] == 13) & (df1['FRACE15'] != 13) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c28.to_csv('D:\\Cornell\\Research\\data\\df1_c28.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f819164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c28 = df2.loc[(df2['MRACE15'] == 13) & (df2['FRACE15'] != 13) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c28.to_csv('D:\\Cornell\\Research\\data\\df2_c28.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a09f7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c28 = df3.loc[(df3['MRACE15'] == 13) & (df3['FRACE15'] != 13) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c28.to_csv('D:\\Cornell\\Research\\data\\df3_c28.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf711f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c28 = r\"D:\\Cornell\\Research\\data\\df1_c28.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c28, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c28 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c28 = r\"D:\\Cornell\\Research\\data\\df2_c28.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c28, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c28 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c28 = r\"D:\\Cornell\\Research\\data\\df3_c28.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c28, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c28 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c28_final = pd.concat([df1_c28, df2_c28, df3_c28], ignore_index = True)\n",
    "c28_final.to_csv('D:\\Cornell\\Research\\data\\c28_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8642d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b181b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c29 = df1.loc[(df1['MRACE15'] == 14) & (df1['FRACE15'] != 14) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c29.to_csv('D:\\Cornell\\Research\\data\\df1_c29.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12cbd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c29 = df2.loc[(df2['MRACE15'] == 14) & (df2['FRACE15'] != 14) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c29.to_csv('D:\\Cornell\\Research\\data\\df2_c29.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f92e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c29 = df3.loc[(df3['MRACE15'] == 14) & (df3['FRACE15'] != 14) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c29.to_csv('D:\\Cornell\\Research\\data\\df3_c29.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02d83e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c29 = r\"D:\\Cornell\\Research\\data\\df1_c29.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c29, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c29 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c29 = r\"D:\\Cornell\\Research\\data\\df2_c29.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c29, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c29 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c29 = r\"D:\\Cornell\\Research\\data\\df3_c29.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c29, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c29 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c29_final = pd.concat([df1_c29, df2_c29, df3_c29], ignore_index = True)\n",
    "c29_final.to_csv('D:\\Cornell\\Research\\data\\c29_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c3429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5675f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c30 = df1.loc[(df1['MRACE15'] == 15) & (df1['FRACE15'] != 15) & (df1['FRACE15'] >= 1) & (df1['FRACE15'] <= 15)]\n",
    "df1_c30.to_csv('D:\\Cornell\\Research\\data\\df1_c30.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52a1dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c30 = df2.loc[(df2['MRACE15'] == 15) & (df2['FRACE15'] != 15) & (df2['FRACE15'] >= 1) & (df2['FRACE15'] <= 15)]\n",
    "df2_c30.to_csv('D:\\Cornell\\Research\\data\\df2_c30.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2598f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_c30 = df3.loc[(df3['MRACE15'] == 15) & (df3['FRACE15'] != 15) & (df3['FRACE15'] >= 1) & (df3['FRACE15'] <= 15)]\n",
    "df3_c30.to_csv('D:\\Cornell\\Research\\data\\df3_c30.csv')\n",
    "# finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d02e87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from local file\n",
    "path_df1_c30 = r\"D:\\Cornell\\Research\\data\\df1_c30.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df1_c30, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df1_c30 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df2_c30 = r\"D:\\Cornell\\Research\\data\\df2_c30.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df2_c30, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df2_c30 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "path_df3_c30 = r\"D:\\Cornell\\Research\\data\\df3_c30.csv\"\n",
    "chunk_size = 1000\n",
    "csv_iterator = pd.read_csv(path_df3_c30, chunksize=chunk_size)\n",
    "chunks = []\n",
    "for chunk in csv_iterator:\n",
    "    chunks.append(chunk)\n",
    "df3_c30 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# combine the three datasets\n",
    "c30_final = pd.concat([df1_c30, df2_c30, df3_c30], ignore_index = True)\n",
    "c30_final.to_csv('D:\\Cornell\\Research\\data\\c30_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e82a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
